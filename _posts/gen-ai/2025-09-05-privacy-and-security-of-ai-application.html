---
layout: post
author: "Rohit Bhardwaj"
categories: gen-ai
tags: ["software testing", "automation", "gen-ai"]
image: "/assets/images/genai/genai.webp"
---

<div class="post-main-container">
  <div class="post-container">
    <h1 class="post-title">
      Data Privacy & Security Concerns in Using AI for QA
    </h1>

    <p>
      Before we dive into using AI applications for QA needs, a common question
      arises:
      <strong
        >How do companies address data leak concerns when sending
        project-related information to AI applications?</strong
      >
      Essentially, how secure are these large language models (LLMs)?
    </p>

    <p>
      When LLMs like ChatGPT first emerged, many companies strictly prohibited
      their use due to privacy concerns; some still do. This raises the
      question: how can we leverage AIâ€™s power when company policies restrict
      its use in projects?
    </p>

    <p>
      The truth is AI applications and policies around them are evolving.
      Organizations are working to strike a balance between data privacy and
      leveraging AI capabilities. Here are some approaches companies are
      adopting or experimenting with:
    </p>

    <ul>
      <li>
        <strong>Hosting Enterprise LLMs On-Premises:</strong> Instead of using
        cloud-hosted services like ChatGPT or Gemini, companies can host
        enterprise-grade LLMs directly within their secured company servers.
        This minimizes exposure and allows strict control over data privacy.
      </li>
      <li>
        <strong>No-Data-Retention Policies:</strong> Some enterprise LLM setups
        apply policies that prevent data submitted during queries from being
        saved or used for further training, ensuring project data remains
        private.
      </li>
      <li>
        <strong>Gateway Controls:</strong> Companies implement filters before
        data reaches the LLM to detect and block sensitive or internal
        information from being sent to external AI servers. This acts as a
        gatekeeper to prevent compromises.
      </li>
      <li>
        <strong>Offline Local Models:</strong> Downloading and running LLMs
        entirely offline on company hardware can offer maximum privacy, though
        this approach demands significant compute resources and infrastructure.
      </li>
    </ul>

    <p>
      It is important to note that
      <span class="highlight"
        >security and infrastructure teams handle the responsibility of
        implementing AI security</span
      >. As QA professionals, our focus is not on securing the AI infrastructure
      but learning how to effectively utilize whatever AI resources (LLMs) are
      available to us within the organization.
    </p>

    <p>
      In this course, we demonstrate concepts using publicly hosted LLMs for
      ease of understanding. However, if your organization provides a secure
      enterprise LLM URL or setup, the same principles apply when interacting
      with AI.
    </p>

    <p>
      Remember, guaranteeing AI security is
      <strong>out of scope for this course</strong>. Our goal is to learn
      leveraging AI to boost testing efficiency once a suitable LLM interface is
      accessible.
    </p>

    <p><em>Thank you for your attention.</em></p>
  </div>
</div>
